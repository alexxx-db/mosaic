{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c3f3354-af76-43e5-9fa3-f7202c692e0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "> Generates the following in database `mosaic_spatial_knn`: (1) table `building_50k`, (2) table `trip_1m`. These are sufficient samples of the full data for this example. __Note:__ You will need to run the actual Spatial KNN on [Databricks ML Runtime](https://docs.databricks.com/en/release-notes/runtime/index.html), for this one it doesn't matter.\n",
    "\n",
    "<p/>\n",
    "\n",
    "1. To use Databricks Labs [Mosaic](https://databrickslabs.github.io/mosaic/index.html) library for geospatial data engineering, analysis, and visualization functionality:\n",
    "  * Install with `%pip install databricks-mosaic`\n",
    "  * Import and use with the following:\n",
    "  ```\n",
    "  import mosaic as mos\n",
    "  mos.enable_mosaic(spark, dbutils)\n",
    "  ```\n",
    "<p/>\n",
    "\n",
    "2. To use [KeplerGl](https://kepler.gl/) OSS library for map layer rendering:\n",
    "  * Already installed with Mosaic, use `%%mosaic_kepler` magic [[Mosaic Docs](https://databrickslabs.github.io/mosaic/usage/kepler.html)]\n",
    "  * Import with `from keplergl import KeplerGl` to use directly\n",
    "\n",
    "If you have trouble with Volume access:\n",
    "\n",
    "* For Mosaic 0.3 series (< DBR 13)     - you can copy resources to DBFS as a workaround\n",
    "* For Mosaic 0.4 series (DBR 13.3 LTS) - you will need to either copy resources to DBFS or setup for Unity Catalog + Shared Access which will involve your workspace admin. Instructions, as updated, will be [here](https://databrickslabs.github.io/mosaic/usage/install-gdal.html).\n",
    "\n",
    "---\n",
    "__Last Updated:__ 27 NOV 2023 [Mosaic 0.3.12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c41f69e4-a93a-4a8b-91d8-94019843bd02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install \"databricks-mosaic<0.4,>=0.3\" --quiet # <- Mosaic 0.3 series\n",
    "# %pip install \"databricks-mosaic<0.5,>=0.4\" --quiet # <- Mosaic 0.4 series (as available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4ea63d-e7b9-4e3f-bed9-63d51b13e50e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -- configure AQE for more compute heavy operations\n",
    "#  - choose option-1 or option-2 below, essential for REPARTITION!\n",
    "# spark.conf.set(\"spark.databricks.optimizer.adaptive.enabled\", False) # <- option-1: turn off completely for full control\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False) # <- option-2: just tweak partition management\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 1_024)                  # <-- default is 200\n",
    "\n",
    "# -- import databricks + spark functions\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# -- setup mosaic\n",
    "import mosaic as mos\n",
    "\n",
    "mos.enable_mosaic(spark, dbutils)\n",
    "# mos.enable_gdal(spark) # <- not needed for this example\n",
    "\n",
    "# --other imports\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e437d01f-add6-4dac-b9cf-1aa9516057ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__Setup Data Location__\n",
    "\n",
    "> You can alter this, of course, to match your preferred location. __Note:__ this is showing DBFS for continuity outside Unity Catalog + Shared Access clusters, but you can easily modify paths to use [Volumes](https://docs.databricks.com/en/sql/language-manual/sql-ref-volumes.html), see more details [here](https://databrickslabs.github.io/mosaic/usage/installation.html) as available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88fbb923-8e5a-4319-b00c-191bb2bbd140",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_name = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "\n",
    "raw_path = f\"dbfs:/{user_name}/geospatial/mosaic/data/spatial_knn\"\n",
    "raw_fuse_path = raw_path.replace(\"dbfs:\",\"/dbfs\")\n",
    "dbutils.fs.mkdirs(raw_path)\n",
    "\n",
    "os.environ['RAW_PATH'] = raw_path\n",
    "os.environ['RAW_FUSE_PATH'] = raw_fuse_path\n",
    "\n",
    "print(f\"The raw data will be stored in '{raw_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98c85386-f334-4570-8857-cd0caac6047f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "building_filename = \"nyc_building_footprints.geojson\"\n",
    "os.environ['BUILDING_FILENAME'] = building_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203ef9df-5ac4-4239-b2c3-3af35a3251fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__Setup Catalog and Schema__\n",
    "\n",
    "> You will have to adjust for your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59419e55-1588-41cf-a191-d210f6e912f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"alexxx_ml_demo\"\n",
    "sql(f\"USE CATALOG {catalog_name}\")\n",
    "\n",
    "db_name = \"mosaic_spatial_knn\"\n",
    "sql(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\n",
    "sql(f\"USE SCHEMA {db_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eb32bde-8575-4190-b8ce-192803905292",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql show tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b82264e3-334c-42c3-a2e4-7ddc9f719d8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup NYC Building Data (`Building` Table | 50K)\n",
    "\n",
    "> While the overall data size is ~1.1M, we are going to just take 50K for purposes of this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f37b525-cf8b-43f6-9cfc-75839ef4b8a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__Download Data (789MB)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb424979-adae-46a2-b89c-ead1e7497813",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pathlib\n",
    "\n",
    "def download_url(data_location, dataset_subpath, url):\n",
    "  fuse_dir = pathlib.Path(data_location.replace('dbfs:',''))\n",
    "  if (\n",
    "    not fuse_dir.name.startswith('/Volumes/') and \n",
    "    not fuse_dir.name.startswith('/Workspace/')\n",
    "  ):\n",
    "    fuse_dir = pathlib.Path(data_location.replace('dbfs:/', '/dbfs/'))\n",
    "  fuse_dir.mkdir(parents=True, exist_ok=True)\n",
    "  fuse_path = fuse_dir / dataset_subpath\n",
    "  if not fuse_path.exists():\n",
    "    req = requests.get(url)\n",
    "    with open(fuse_path, 'wb') as f:\n",
    "      f.write(req.content)\n",
    "  else:\n",
    "    print(f\"'{fuse_path}' exists...skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "834611da-2dce-4738-b9f7-38aae360c473",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# buildings - data preview = https://data.cityofnewyork.us/Housing-Development/Building-Footprints/nqwf-w8eh\n",
    "download_url(raw_path, building_filename, \"https://data.cityofnewyork.us/api/geospatial/nqwf-w8eh?method=export&format=GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "145be74e-b1f6-4e78-a434-8076844ee721",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ls -lh $RAW_FUSE_PATH/$BUILDING_FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1fc720f-b7b3-4a25-a89b-072c16e7310b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__Generate DataFrame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "362a23a3-f8e4-45fd-8bd3-737767dcfa7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def fix_geojson(gj_dict):\n",
    "  \"\"\"\n",
    "  This GeoJSON has coordinates nested as a string, \n",
    "  so standardize here to avoid issues, gets to same as\n",
    "  expected when `to_json(\"feature.geometry\")` is\n",
    "  normally called.\n",
    "  \"\"\"\n",
    "  import json\n",
    "  \n",
    "  r_list = []\n",
    "  for l in gj_dict['coordinates']:\n",
    "    if isinstance(l,str):\n",
    "      r_list.append(json.loads(l))\n",
    "    else:\n",
    "      r_list.append(l)\n",
    "  \n",
    "  return json.dumps(\n",
    "    {\n",
    "      \"type\": gj_dict['type'],\n",
    "      \"coordinates\": r_list\n",
    "    }\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f7583c-0f39-4e53-be99-e477f389e772",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.clearCache() # <- cache useful for dev (avoid recomputes)\n",
    "\n",
    "_df_geojson_raw = (\n",
    "  spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .format(\"json\")\n",
    "    .load(f\"{raw_path}/{building_filename}\")\n",
    "      .select(\"type\", F.explode(col(\"features\")).alias(\"feature\"))\n",
    "      .repartition(24)\n",
    "        .select(\n",
    "          \"type\", \n",
    "          \"feature.properties\", \n",
    "          fix_geojson(\"feature.geometry\").alias(\"json_geometry\")\n",
    "        )\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "print(f\"count? {_df_geojson_raw.count():,}\")\n",
    "display(_df_geojson_raw.limit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc16e0de-2a44-4abd-90fb-eee3e49111ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_df_geojson = (\n",
    "  _df_geojson_raw\n",
    "    .withColumn(\"geom\", mos.st_geomfromgeojson(\"json_geometry\"))\n",
    "    .withColumn(\"geom_wkt\", mos.st_astext(\"geom\"))\n",
    "    .withColumn(\"is_valid\", mos.st_isvalid(\"geom_wkt\"))\n",
    "    .select(\"properties.*\", \"geom_wkt\", \"is_valid\")\n",
    ")\n",
    "\n",
    "# print(f\"count? {_df_geojson.count():,}\")\n",
    "# display(_df_geojson.limit(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07449a49-1763-4127-b486-ece94d89fc37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__Get Sample of 50K__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e9d0289-b285-4427-9649-1f8b4469546e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_df_geojson_50k = (\n",
    "  _df_geojson\n",
    "    .sample(0.05)\n",
    "    .limit(50_000)\n",
    ")\n",
    "\n",
    "print(f\"count? {_df_geojson_50k.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20532979-c8e1-4f79-a637-6f23293091d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "__Write out to Delta Lake__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd86cab5-f718-410d-a2ff-7d168c5c4c34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "  _df_geojson_50k\n",
    "    .write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .saveAsTable(f\"building_50k\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5052760-55fc-400c-8cd0-f5d0c204d030",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select format_number(count(1), 0) as count from building_50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1868cd58-c401-41df-86dc-8700bffce9d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select * from building_50k limit 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5815c93b-9e89-458a-9ae3-8623d4ca23e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup NYC Taxi Data (`taxi_trip` | 1M)\n",
    "\n",
    "> This data is available as part of `databricks-datasets` for customer. We are just going to take 1M trips for our purposes.\n",
    "\n",
    "__Will write sample out to Delta Lake__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8025bdfa-d10d-41e5-ba8e-6589e41295cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "  spark.table(\"delta.`/databricks-datasets/nyctaxi/tables/nyctaxi_yellow`\")\n",
    "    .sample(0.001)\n",
    "  .withColumn(\n",
    "    \"pickup_point\", mos.st_aswkt(mos.st_point(F.col(\"pickup_longitude\"), F.col(\"pickup_latitude\")))\n",
    "  )\n",
    "  .withColumn(\n",
    "    \"dropoff_point\", mos.st_aswkt(mos.st_point(F.col(\"dropoff_longitude\"), F.col(\"dropoff_latitude\")))\n",
    "  )\n",
    "  .limit(1_000_000)\n",
    "  .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(f\"taxi_trip_1m\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "724edd96-e852-485f-8369-234d3aba8dd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select format_number(count(1), 0) as count from taxi_trip_1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef9ca8a5-8c77-4b84-8a6a-3d0a3ccae64e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select * from taxi_trip_1m limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3912f0-89af-4454-a8fd-44489e6c3736",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "100931ad-f0b2-4e11-a6cc-01a87652c28a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql show tables from mosaic_spatial_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da89da17-dbdf-4e4c-9729-b1b557f77e60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "-- notice this is a managed table (see 'Location' col_name)\n",
    "describe table extended building_50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20012234-f169-4f80-b613-4418defb56da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "-- notice this is a managed table (see 'Location' col_name)\n",
    "describe table extended taxi_trip_1m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e68d70ce-31d0-45bc-99a2-91b46f8ed7c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Optional: Clean up initial GeoJSON\n",
    "\n",
    "> Now that the building data (sample) is in Delta Lake, we don't need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b003da-e31f-4a47-91a8-cb473ff566c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(raw_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49f5ac2a-4ed0-48bb-9e84-3d32755ab4c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -- uncomment to remove geojson file --\n",
    "# dbutils.fs.rm(f\"{raw_path}/{building_filename}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2308237406562728,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01. Data Prep",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
