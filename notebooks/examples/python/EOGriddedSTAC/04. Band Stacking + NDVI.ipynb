{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efdde798-21d5-4d43-9925-b78e59dd09eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Band Stacking + NDVI\n",
    "\n",
    "> Making use of H3 global grid indexing when working with rasters. For this example series, we focus on B04 (red), B03 (green), B02 (blue), and B08 (nir).\n",
    "\n",
    "---\n",
    "__Last Update:__ 16 JAN 2024 [Mosaic 0.3.14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a5cec22-2274-4d1c-8a07-37705ef61354",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup Notes\n",
    "\n",
    "> Raster processing is memory intensize. Here are some config tips.\n",
    "\n",
    "* This notebook was run on AWS [r5d.4xlarge](https://www.databricks.com/product/pricing/product-pricing/instance-types) instances (1-10 workers auto-scaling for up to 160 concurrent tasks).\n",
    "* __Optional:__ Prior to launching, the following can be added to the cluster spark configs, adapted from this databricks [blog](https://www.databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html) and these JVM tuning [docs](https://docs.oracle.com/cd/E40972_01/doc.70/e40973/cnf_jvmgc.htm#autoId2): `spark.executor.extraJavaOptions -XX:+UseG1GC -XX:+PrintFlagsFinal -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=32M -XX:ParallelGCThreads=16 -XX:ConcGCThreads=4`. _This optimization is not required for this example._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd38fdab-6f32-434b-b64e-c899e8f0eefd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Imports + Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "590cc0ab-2293-4d2d-80b8-8a3e000090b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet 'databricks-mosaic<0.4,>=0.3'\n",
    "%pip install --quiet databricks-mosaic rasterio==1.3.5 gdal==3.4.3 pystac pystac_client planetary_computer tenacity rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d91f0cb5-0176-49d2-943c-baa04236e6de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -- configure AQE for more compute heavy operations\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 512)\n",
    "\n",
    "# -- import databricks + delta + spark functions\n",
    "from delta.tables import *\n",
    "from pyspark.databricks.sql import functions as dbf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# -- setup mosaic\n",
    "import mosaic as mos\n",
    "\n",
    "mos.enable_mosaic(spark, dbutils)\n",
    "mos.enable_gdal(spark)\n",
    "\n",
    "# -- other imports\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "from matplotlib import pyplot\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "import library\n",
    "import os\n",
    "import pathlib\n",
    "import planetary_computer\n",
    "import pystac_client\n",
    "import rasterio\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cf3a1ff-1eb2-470f-9518-c9d62693e484",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mos.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30ca4f28-a7e0-4300-ac3d-be769197b152",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fc5d315-2472-494f-ad51-ab356dc2dd06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Databricks Catalog + Schema\n",
    "\n",
    "> This is for reading and writing out table(s). __Note: these should already exists after running notebook 01.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2a350af-21e1-46a7-aca4-4c546147e4b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# adjust to match your catalog + schema\n",
    "catalog_name = \"geospatial_docs\"\n",
    "db_name = \"eo_alaska\"\n",
    "\n",
    "sql(f\"\"\"USE CATALOG {catalog_name}\"\"\")\n",
    "sql(f\"\"\"USE DATABASE {db_name}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85faa036-3622-492b-9479-d2a4ebb07d33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71056ad9-7490-48e3-9998-0966423c5114",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_We are focusing on the gridded tables for band stacking and NDVI._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5014751c-7bd4-41e2-86cb-4a0a25a37722",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04f8e5c-1504-442a-9d83-b40a9dd20c10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_b02_h3 = spark.read.table(\"band_b02_h3\")\n",
    "df_b03_h3 = spark.read.table(\"band_b03_h3\")\n",
    "df_b04_h3 = spark.read.table(\"band_b04_h3\")\n",
    "df_b08_h3 = spark.read.table(\"band_b08_h3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d60c8ae-be5b-4378-a7a7-1ada533d8d73",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Band Stacking\n",
    "\n",
    "> We are going to merge bands into rasters from the same h3 cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8c0f458-1391-4bd6-94c1-dfe920d1cd43",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_[1] Consolidate to 1 tile per H3 cell per day with `rst_merge_agg` aggregate function._ __Note: the following shows the multiple tiles in some cells.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6228a69-29c4-4d7c-9f3a-f0dc862a5563",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "  df_b02_h3.groupBy(\"index_id\", \"date\", \"band_name\")\n",
    "    .count()\n",
    "  .orderBy(F.desc(\"count\"))\n",
    "  .limit(25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab6aa05-47a9-473d-a252-d6be10d47825",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_b02_resolved = df_b02_h3.groupBy(\"index_id\", \"date\", \"band_name\")\\\n",
    "  .agg(mos.rst_merge_agg(\"tile\").alias(\"tile\"))\n",
    "\n",
    "df_b03_resolved = df_b03_h3.groupBy(\"index_id\", \"date\", \"band_name\")\\\n",
    "  .agg(mos.rst_merge_agg(\"tile\").alias(\"tile\"))\n",
    "\n",
    "df_b04_resolved = df_b04_h3.groupBy(\"index_id\", \"date\", \"band_name\")\\\n",
    "  .agg(mos.rst_merge_agg(\"tile\").alias(\"tile\"))\n",
    "\n",
    "df_b08_resolved = df_b08_h3.groupBy(\"index_id\", \"date\", \"band_name\")\\\n",
    "  .agg(mos.rst_merge_agg(\"tile\").alias(\"tile\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd7e68a7-30d0-46e9-8ff8-4cda9c83f9af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_Here is example of the resolved output._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b89dcf0-b714-47a6-9eb7-75a527079f5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"band 02 resolved count? {df_b02_resolved.count():,}\")\n",
    "df_b02_resolved.limit(10).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1c99ab8-08ee-49ae-bd51-62392f507ebe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_[2] Band stack per H3 tile; in this case, red, green, blue, and nir._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4729475-4a95-4c46-a084-1002bccf3045",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_[a] Here is a brief example of using union and window to stack by (ordered) band_name._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "614df173-c5c5-4949-bd89-4661e9f3cde9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stack_win = Window.partitionBy(\"index_id\",\"date\").orderBy('band_name')\n",
    "\n",
    "display (\n",
    "  df_b02_resolved\n",
    "    .filter(\"index_id = 608197214103142399\")\n",
    "  .union(\n",
    "    df_b03_resolved\n",
    "      .filter(\"index_id = 608197214103142399\")\n",
    "  )\n",
    "  .union(\n",
    "    df_b04_resolved\n",
    "      .filter(\"index_id = 608197214103142399\")\n",
    "  )\n",
    "  .union(\n",
    "    df_b08_resolved\n",
    "      .filter(\"index_id = 608197214103142399\")\n",
    "  )\n",
    "  .select(\n",
    "    \"date\",\n",
    "    F\n",
    "      .collect_list(\"tile\")\n",
    "        .over(stack_win)\n",
    "      .alias(\"tiles\")\n",
    "  )\n",
    "  .filter(\"array_size(tiles) = 4\")\n",
    "  .withColumn(\"tile\", mos.rst_frombands(\"tiles\"))\n",
    "  .withColumn(\"tile\", mos.rst_initnodata(\"tile\"))\n",
    "  .withColumn(\"memsize\", mos.rst_memsize(\"tile\"))\n",
    "  .withColumn(\"num_bands\", mos.rst_numbands(\"tile\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4eb5c2a-1b22-4388-9a94-accea0a77b44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_[b] Here is the main example, where we join new columns for each band to generate a stacked raster (in the order we choose)._\n",
    "\n",
    "> Hint: joins default to inner, if you want something different add arg 'how'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f5d9c5-2532-45b7-946b-c303c882f53a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stack_tbl_name = \"band_stack\"\n",
    "# sql(f\"\"\"drop table if exists {stack_tbl_name}\"\"\") # <- uncomment to drop\n",
    "if not spark.catalog.tableExists(stack_tbl_name):\n",
    "  try:\n",
    "    repartition_factor = 100 # <- num tiles per task\n",
    "    orig_repart_num = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "    repart_num = round(df_b02_resolved.count() / repartition_factor)\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", repart_num)\n",
    "    print(f\"\\t...shuffle partitions to {repart_num} for this operation.\")\n",
    "    (\n",
    "      df_b02_resolved\n",
    "        .drop(\"band_name\")\n",
    "        .repartition(repart_num, \"tile\")\n",
    "        .withColumnRenamed(\"tile\", \"b02\")\n",
    "        .join(\n",
    "          df_b03_resolved\n",
    "            .drop(\"band_name\")\n",
    "            .repartition(repart_num, \"tile\")\n",
    "            .withColumnRenamed(\"tile\", \"b03\"),\n",
    "          on = [\"index_id\", \"date\"]\n",
    "        )\n",
    "        .join(\n",
    "          df_b04_resolved\n",
    "            .drop(\"band_name\")\n",
    "            .repartition(repart_num, \"tile\")\n",
    "            .withColumnRenamed(\"tile\", \"b04\"),\n",
    "          on = [\"index_id\", \"date\"]\n",
    "        )\n",
    "        .join(\n",
    "          df_b08_resolved\n",
    "            .drop(\"band_name\")\n",
    "            .repartition(repart_num, \"tile\")\n",
    "            .withColumnRenamed(\"tile\", \"b08\"),\n",
    "          on = [\"index_id\", \"date\"]\n",
    "        )\n",
    "        .withColumn(\"tile\", mos.rst_frombands(F.array(\"b04\",\"b03\",\"b02\",\"b08\"))) \n",
    "        .withColumn(\"tile\", mos.rst_initnodata(\"tile\"))\n",
    "        .withColumn(\"memsize\", mos.rst_memsize(\"tile\"))\n",
    "        .withColumn(\"num_bands\", mos.rst_numbands(\"tile\"))\n",
    "        #.drop(\"b04\", \"b03\", \"b02\", \"b08\") <- keep for now\n",
    "      .write\n",
    "      .saveAsTable(stack_tbl_name)\n",
    "    )\n",
    "  finally:\n",
    "    # print(f\"...setting shuffle partitions back to {orig_repart_num}\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", orig_repart_num)\n",
    "  \n",
    "sql(f\"\"\"OPTIMIZE {stack_tbl_name} ZORDER BY (index_id)\"\"\")\n",
    "stacked_df = spark.table(stack_tbl_name)\n",
    "print(f\"count? {stacked_df.count():,}\")\n",
    "stacked_df.limit(1).show(vertical=True) # <- show + limiting for ipynb only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5acb605e-63e8-4651-86af-09196da5b928",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "json.loads(stacked_df.filter(\"index_id = 608197214103142399\").select(mos.rst_summary(\"tile\")).first()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "807daf57-d115-4e21-90cc-e4d0576f43ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "library.plot_raster(stacked_df.select(\"tile\", \"memsize\").filter(\"memsize > 400000\").first()[\"tile\"][\"raster\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "182c7b75-d38e-4d6e-9b9a-e4a57f015525",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## NDVI Calculation\n",
    "\n",
    "> Now that we have our band-stacked tiles, using mosaic's `rst_ndvi` we can calculate [NDVI](https://gisgeography.com/ndvi-normalized-difference-vegetation-index/) from the red and nir bands. _Note: we could also use mosaic's more flexible `rst_mapalgebra` which wraps [gdal_calc](https://gdal.org/programs/gdal_calc.html)._\n",
    "\n",
    "__NDVI over full data can take multiple hours, recommend filtering as shown below (`filter(col(\"item_id\").contains(\"_R014_\"))`) will shorten the processing time.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20da6688-474d-4722-aaf9-ab92643899a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ndvi_tbl_name = \"ndvi_calc\"\n",
    "# sql(f\"\"\"drop table if exists {ndvi_tbl_name}\"\"\") # <- uncomment to drop\n",
    "if not spark.catalog.tableExists(ndvi_tbl_name):\n",
    "  repartition_factor = 100\n",
    "  (\n",
    "    stacked_df\n",
    "      #.filter(col(\"item_id\").contains(\"_R014_\"))  # <- uncomment to filter \n",
    "      .repartition(round(stacked_df.count() / repartition_factor), \"tile\")\n",
    "      .withColumn(\"ndvi\", mos.rst_ndvi(\"tile\", F.lit(1), F.lit(4))) # 01 = red, 04 = nir\n",
    "    .write\n",
    "    .saveAsTable(ndvi_tbl_name)\n",
    "  )\n",
    "sql(f\"\"\"OPTIMIZE {ndvi_tbl_name} ZORDER BY (index_id)\"\"\")\n",
    "ndvi_df = spark.table(ndvi_tbl_name)\n",
    "print(f\"count? {ndvi_df.count():,}\")\n",
    "ndvi_df.limit(1).show(vertical=True) # <- show + limiting for ipynb only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42bedaff-7269-48e4-b76f-9531e6fdf1e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "to_plot = ndvi_df.limit(50).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d6f5c89-fc28-423a-930c-7b2b26f4fa89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "library.plot_raster(to_plot[4][\"ndvi\"][\"raster\"])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 69050624926671,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "04. Band Stacking + NDVI",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
