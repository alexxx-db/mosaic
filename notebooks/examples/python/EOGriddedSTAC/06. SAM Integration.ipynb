{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e84f666-a037-464f-894b-09bfc729e5e5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Segment Anything Model (SAM) Integration\n",
    "\n",
    "> Making use of H3 global grid indexing when working with rasters. For this example series, we focus on B04 (red), B03 (green), B02 (blue), and B08 (nir).\n",
    "\n",
    "---\n",
    "__Last Update:__ 17 JAN 2024 [Mosaic 0.3.14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12c764ed-3013-43f5-b309-acff5613ce90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup Notes\n",
    "\n",
    "> Raster processing is memory intensize. Here are some config tips.\n",
    "\n",
    "* This notebook was run on AWS [r5d.4xlarge](https://www.databricks.com/product/pricing/product-pricing/instance-types) instances (1-10 workers auto-scaling for up to 160 concurrent tasks).\n",
    "* __Optional:__ Prior to launching, the following can be added to the cluster spark configs, adapted from this databricks [blog](https://www.databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html) and these JVM tuning [docs](https://docs.oracle.com/cd/E40972_01/doc.70/e40973/cnf_jvmgc.htm#autoId2): `spark.executor.extraJavaOptions -XX:+UseG1GC -XX:+PrintFlagsFinal -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=32M -XX:ParallelGCThreads=16 -XX:ConcGCThreads=4`. _This optimization is not required for this example._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f98e3311-7ba9-48e3-9edb-0771bd17723f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Imports + Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34de873b-82dd-42be-8865-2fd58edf813e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet 'databricks-mosaic<0.4,>=0.3' \n",
    "%pip install --quiet databricks-mosaic rasterio==1.3.5 gdal==3.4.3 pystac pystac_client planetary_computer tenacity rich\n",
    "%pip install --quiet torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee690941-1ee0-436c-9a3d-7f62fe37e32a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -- configure AQE for more compute heavy operations\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", False)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 512)\n",
    "\n",
    "# -- import databricks + delta + spark functions\n",
    "from delta.tables import *\n",
    "from pyspark.databricks.sql import functions as dbf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, pandas_udf, udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# -- setup mosaic\n",
    "import mosaic as mos\n",
    "\n",
    "mos.enable_mosaic(spark, dbutils)\n",
    "mos.enable_gdal(spark)\n",
    "\n",
    "# -- SAM imports\n",
    "from PIL import Image\n",
    "from rasterio.plot import reshape_as_raster, reshape_as_image\n",
    "from transformers import SamModel, SamProcessor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sam_lib\n",
    "import torch\n",
    "\n",
    "# -- other imports\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "from matplotlib import pyplot\n",
    "from rasterio.io import MemoryFile\n",
    "\n",
    "import library\n",
    "import os\n",
    "import pathlib\n",
    "import planetary_computer\n",
    "import pystac_client\n",
    "import rasterio\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29521f9e-3c43-48aa-943c-03cd0d965259",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mos.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9959be65-6ade-4f65-a519-62f4b3123091",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext library\n",
    "%reload_ext sam_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "098da9bd-ffd6-4004-84e0-4bff5815bf8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Databricks Catalog + Schema\n",
    "\n",
    "> This is for reading and writing out table(s). __Note: these should already exists after running notebook 01.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c60191ea-1404-48a4-ba41-670fa18286ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# adjust to match your catalog + schema\n",
    "catalog_name = \"geospatial_docs\"\n",
    "db_name = \"eo_alaska\"\n",
    "\n",
    "sql(f\"\"\"USE CATALOG {catalog_name}\"\"\")\n",
    "sql(f\"\"\"USE DATABASE {db_name}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6b6c266-7506-4cf0-8e59-b6b2c8a11dac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data + Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4264c055-6683-43ad-8b77-947a96713d6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql show tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee8d69e6-1d3b-437f-9cec-01a088474b2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "b02_h3_df = spark.read.table(\"band_b02_h3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5fb8144-ac97-4344-8257-b48a983ef177",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "library.plot_raster(b02_h3_df.limit(50).collect()[0][\"tile\"][\"raster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ab38c7e-18c1-47ac-8ac0-961dbe91fd08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "b02_h3_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf352d1-52d0-480e-af6c-4b533b01edb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/facebook/sam-vit-huge\n",
    "\n",
    "def get_device():\n",
    "   return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_sam_processor():\n",
    "  return SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n",
    "\n",
    "def load_sam_model(device=get_device()):\n",
    "  return SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(device)\n",
    "\n",
    "sam_model = load_sam_model()\n",
    "sam_processor = load_sam_processor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc223966-a247-4a77-ab7f-8fb4b482c2b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Apply SAM on one of the tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b864c9-d783-4d27-8452-5ce65a44e276",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tiles = (\n",
    "  b02_h3_df\n",
    "  .select(\"tile.*\")\n",
    "  .limit(50)\n",
    "  .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5fcbd5e-aa33-4c94-aec7-b7cd69930ec0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raster = tiles[1][\"raster\"]\n",
    "library.plot_raster(raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85383f09-0c60-4df4-88b0-fe16867bbe2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def raster_to_image(raster, band_num=1):\n",
    "  \"\"\"\n",
    "  Reshape the provided raster for PIL.\n",
    "  Adapted from https://rasterio.readthedocs.io/en/stable/topics/image_processing.html\n",
    "  \"\"\"\n",
    "  try:\n",
    "    np_raster = library.to_numpy_arr(raster).astype(np.uint8)\n",
    "    np_img = reshape_as_image(np_raster)\n",
    "    np_img1 = np_img[:,:, band_num - 1]\n",
    "    return Image.fromarray(np_img1)\n",
    "  except:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e70de2-84b2-4da4-9cc8-55ce1c825190",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_image = raster_to_image(raster, band_num=1)\n",
    "raw_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d51c5c78-49df-483d-ad84-6272affeaff0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_masks(raw_image, model=sam_model, processor=sam_processor, device=get_device()):\n",
    "  inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "  image_embeddings = model.get_image_embeddings(inputs[\"pixel_values\"])\n",
    "  inputs.pop(\"pixel_values\", None)\n",
    "  inputs.update({\"image_embeddings\": image_embeddings})\n",
    "\n",
    "  with torch.no_grad():\n",
    "    outputs = sam_model(**inputs)\n",
    "\n",
    "  masks = processor.image_processor.post_process_masks(\n",
    "    outputs.pred_masks.cpu(),\n",
    "    inputs[\"original_sizes\"].cpu(),\n",
    "    inputs[\"reshaped_input_sizes\"].cpu()\n",
    "  )\n",
    "  return masks\n",
    "  \n",
    "def get_scores(raw_image, model=sam_model, processor=sam_processor, device=get_device()):\n",
    "  inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "  image_embeddings = model.get_image_embeddings(inputs[\"pixel_values\"])\n",
    "  inputs.pop(\"pixel_values\", None)\n",
    "  inputs.update({\"image_embeddings\": image_embeddings})\n",
    "\n",
    "  with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "  scores = outputs.iou_scores\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "256ccf3c-2232-43be-96ad-c3e1ec7f5f38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scores = get_scores(raw_image)\n",
    "masks = get_masks(raw_image)\n",
    "print(\"scores ->\", scores)\n",
    "print(\"masks ->\", masks[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc415928-7b47-47dd-99c1-bef676e615e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sam_lib.show_masks_on_image(raw_image, masks[0], scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf63b981-9e2a-4580-b516-4e736bb5807a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Scaling model scoring with pandas UDFs\n",
    "\n",
    "> This is just showing a subset to give an example of the pattern. You can limit further, e.g. `limit(10)`, or can open up to the entire dataset if you like. Running SAM on the specified notebook config took about 10 minutes for 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "861dcd53-cfcd-4750-b4ad-16cc32ee1598",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@pandas_udf(\"array<double>\")\n",
    "def apply_sam(rasters: pd.Series) -> pd.Series:\n",
    "  return rasters\\\n",
    "    .apply(lambda raster: raster_to_image(raster))\\\n",
    "    .apply(lambda image: get_scores(image).flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef0450f-2915-47ff-8a58-2bcc180e76c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "  b02_h3_df\n",
    "  .select(\"item_id\",\"tile\")\n",
    "  .limit(256)\n",
    "  .repartition(256, \"tile\")\n",
    "  .withColumn(\"model_result\", apply_sam(F.col(\"tile.raster\")))\n",
    "  .display()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 69050624847620,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "06. SAM Integration",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
